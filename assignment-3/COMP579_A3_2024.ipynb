{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALnFQQHn1QME"
   },
   "source": [
    "# COMP579 Assignment 2\n",
    "\n",
    "**Coding: Tabular RL [70 points]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VlHR8XcY483f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4KZyIS-5LxK",
    "outputId": "e074b368-abb2-49f2-c3fd-851279cdb8ed"
   },
   "outputs": [],
   "source": [
    "def softmax(x, temp=1.0):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X with temperature adjustment.\n",
    "    When temp is 0, the function returns an argmax-like result.\n",
    "\n",
    "    Parameters:\n",
    "    x (array_like): Input data.\n",
    "    temp (float, optional): Temperature parameter to adjust the sharpness of softmax. Default is 1.0.\n",
    "\n",
    "    Returns:\n",
    "    array_like: Softmax-transformed output with the same shape as input x.\n",
    "    \"\"\"\n",
    "    if temp == 0:\n",
    "        # Simulate argmax behavior for zero temperature\n",
    "        return np.equal(x, np.max(x, axis=-1, keepdims=True)).astype(float)\n",
    "    else:\n",
    "        # Adjust for temperature and improve numerical stability in one step\n",
    "        z = (x - np.max(x, axis=-1, keepdims=True)) / temp\n",
    "        top = np.exp(z)\n",
    "        bottom = np.sum(top, axis=-1, keepdims=True)\n",
    "        return top / bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    \"\"\"\n",
    "    A class representing the SARSA learning algorithm.\n",
    "\n",
    "    Attributes:\n",
    "    env (gym.Env): The environment to interact with.\n",
    "    alpha (float): The learning rate.\n",
    "    gamma (float): The discount factor for future rewards.\n",
    "    temp (float): The temperature parameter for softmax action selection.\n",
    "    Q (numpy.ndarray): The Q-table for storing state-action values.\n",
    "\n",
    "    Methods:\n",
    "    select_action(s, action_mask, greedy=False): Selects an action based on the current policy.\n",
    "    update(s, a, r, s_prime, a_prime, done): Updates the Q-table based on the observed transition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, alpha, gamma, temp):\n",
    "        \"\"\"\n",
    "        Initializes the SARSA agent.\n",
    "\n",
    "        Parameters:\n",
    "        env (gym.Env): The environment to interact with.\n",
    "        alpha (float): The learning rate.\n",
    "        gamma (float): The discount factor for future rewards.\n",
    "        temp (float): The temperature parameter for softmax action selection.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.temp = temp\n",
    "        self.Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    def select_action(self, s, action_mask, greedy=False):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current policy.\n",
    "\n",
    "        Parameters:\n",
    "        s (int): The current state.\n",
    "        action_mask (numpy.ndarray): A mask indicating allowed actions in the current state.\n",
    "        greedy (bool): If True, selects the action with the highest Q-value.\n",
    "\n",
    "        Returns:\n",
    "        int: The selected action.\n",
    "        \"\"\"\n",
    "        # Filter out actions that are not allowed in the current state\n",
    "        allowed = np.where(action_mask == 1)[0]\n",
    "        # Extract Q-values for the allowed actions\n",
    "        Q_actions = self.Q[s, allowed]\n",
    "        \n",
    "        temp = 0 if greedy else self.temp\n",
    "\n",
    "        # Otherwise, select an action based on a softmax distribution\n",
    "        return random.choices(allowed, softmax(Q_actions, temp=temp), k=1)[0]\n",
    "\n",
    "    \n",
    "    def update(self, s, a, r, s_prime, a_prime, done, action_mask=None):\n",
    "        \"\"\"\n",
    "        Updates the Q-table based on the observed transition.\n",
    "\n",
    "        Parameters:\n",
    "        s (int): The current state.\n",
    "        a (int): The action taken.\n",
    "        r (float): The reward received.\n",
    "        s_prime (int): The next state.\n",
    "        a_prime (int): The action taken in the next state.\n",
    "        done (bool): Whether the episode has ended.\n",
    "        action_mask (numpy.ndarray): A mask indicating allowed actions in state s_prime.\n",
    "\n",
    "        Returns:\n",
    "        tuple: The next state and action.\n",
    "        \"\"\"\n",
    "        # Calculate the Q-value for the next state-action pair if not in a terminal state\n",
    "        Q_prime = 0 if done else self.Q[s_prime, a_prime]\n",
    "        # Update the Q-value for the current state-action pair\n",
    "        self.Q[s, a] += self.alpha * (r + self.gamma * Q_prime - self.Q[s, a])\n",
    "        return s_prime, a_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gttYBrHj5Yal"
   },
   "outputs": [],
   "source": [
    "class ExpectedSarsa:\n",
    "    \"\"\"\n",
    "    A class representing the SARSA learning algorithm.\n",
    "\n",
    "    Attributes:\n",
    "    env (gym.Env): The environment to interact with.\n",
    "    alpha (float): The learning rate.\n",
    "    gamma (float): The discount factor for future rewards.\n",
    "    temp (float): The temperature parameter for softmax action selection.\n",
    "    Q (numpy.ndarray): The Q-table for storing state-action values.\n",
    "\n",
    "    Methods:\n",
    "    select_action(s, action_mask, greedy=False): Selects an action based on the current policy.\n",
    "    update(s, a, r, s_prime, a_prime, done): Updates the Q-table based on the observed transition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, alpha, gamma, temp):\n",
    "        \"\"\"\n",
    "        Initializes the SARSA agent.\n",
    "\n",
    "        Parameters:\n",
    "        env (gym.Env): The environment to interact with.\n",
    "        alpha (float): The learning rate.\n",
    "        gamma (float): The discount factor for future rewards.\n",
    "        temp (float): The temperature parameter for softmax action selection.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.temp = temp\n",
    "        self.Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    def select_action(self, s, action_mask, greedy=False):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current policy.\n",
    "\n",
    "        Parameters:\n",
    "        s (int): The current state.\n",
    "        action_mask (numpy.ndarray): A mask indicating allowed actions in the current state.\n",
    "        greedy (bool): If True, selects the action with the highest Q-value.\n",
    "\n",
    "        Returns:\n",
    "        int: The selected action.\n",
    "        \"\"\"\n",
    "        # Filter out actions that are not allowed in the current state\n",
    "        allowed = np.where(action_mask == 1)[0]\n",
    "        # Extract Q-values for the allowed actions\n",
    "        Q_actions = self.Q[s, allowed]\n",
    "\n",
    "        temp = 0 if greedy else self.temp\n",
    "        \n",
    "        # Otherwise, select an action based on a softmax distribution\n",
    "        return random.choices(allowed, softmax(Q_actions, temp=temp), k=1)[0]\n",
    "    \n",
    "    def update(self, s, a, r, s_prime, a_prime, done, action_mask):\n",
    "        \"\"\"\n",
    "        Updates the Q-table based on the observed transition.\n",
    "    \n",
    "        Parameters:\n",
    "        s (int): The current state.\n",
    "        a (int): The action taken.\n",
    "        r (float): The reward received.\n",
    "        s_prime (int): The next state.\n",
    "        a_prime (int): The action taken in the next state.\n",
    "        done (bool): Whether the episode has ended.\n",
    "        action_mask (numpy.ndarray): A mask indicating allowed actions in state s_prime.\n",
    "        \"\"\"\n",
    "        # Calculate the Q-value for the next state-action pair if not in a terminal state\n",
    "        if done:\n",
    "            target = r\n",
    "        else:\n",
    "            # Apply the action mask to filter out disallowed actions\n",
    "            allowed_actions = np.where(action_mask == 1)[0]\n",
    "            Q_prime = self.Q[s_prime, allowed_actions]\n",
    "            pi = softmax(Q_prime, self.temp)\n",
    "            \n",
    "            # Calculate the expected value considering only allowed actions\n",
    "            expected_q = np.sum(pi * Q_prime)\n",
    "            target = r + self.gamma * expected_q\n",
    "    \n",
    "        self.Q[s, a] += self.alpha * (target - self.Q[s, a])\n",
    "        return s_prime, a_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gttYBrHj5Yal"
   },
   "outputs": [],
   "source": [
    "# # bonus question, optional\n",
    "# class Hybrid_Sarsa_Q:\n",
    "#   def __init__(self, env, alpha, gamma, temp):\n",
    "#     # write your solution here\n",
    "#     self.env = None\n",
    "#     self.alpha = None\n",
    "#     self.gamma = None\n",
    "#     self.temp = None\n",
    "#     self.Q = None\n",
    "#     return\n",
    "\n",
    "#   def select_action(self, s, greedy=False):\n",
    "#     # write your solution here\n",
    "#     if greedy:\n",
    "#       # if finished training, then choose the optimal policy\n",
    "#       return\n",
    "#     else:\n",
    "#       return\n",
    "\n",
    "#   def update(self, s, a, r, s_prime, a_prime, done):\n",
    "#     # write your solution here\n",
    "#     return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sazk2zxT8jD7"
   },
   "source": [
    "# Write your experiment code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(args, env=None):\n",
    "    \"\"\"\n",
    "    Runs a single trial of the given RL-algorithm on a given environment.\n",
    "\n",
    "    Parameters:\n",
    "    temp (float): Temperature parameter for model.\n",
    "    alpha (float): Learning rate for model.\n",
    "    gamma (float): Discount factor for model.\n",
    "    n_segments (int): Number of segments to divide the trial into.\n",
    "    n_episodes (int): Number of episodes per segment.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the history of training and testing rewards.\n",
    "    \"\"\"\n",
    "    model_name, run, trial, temp, alpha, gamma, n_segments, n_episodes = args\n",
    "\n",
    "    if env is None:\n",
    "        env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "    model_dict = dict(sarsa=Sarsa, expected_sarsa=ExpectedSarsa)\n",
    "        \n",
    "    model = model_dict[model_name](env, alpha=alpha, gamma=gamma, temp=temp)\n",
    "    train_history = []\n",
    "    test_history = []\n",
    "\n",
    "    # Outer loop for segments with progress bar\n",
    "    for segment in range(n_segments):\n",
    "        # Training episodes for this segment\n",
    "        for episode in range(n_episodes):\n",
    "            s, info = env.reset()\n",
    "            total_reward = 0\n",
    "            for step in range(200):\n",
    "                action_mask = info.get(\"action_mask\", None)\n",
    "                a = model.select_action(s, action_mask)\n",
    "                s_prime, r, done, trunc, info = env.step(a)\n",
    "                total_reward += r\n",
    "                if done:\n",
    "                    break\n",
    "                action_mask = info.get(\"action_mask\", None)\n",
    "                a_prime = model.select_action(s_prime, action_mask)\n",
    "                s, a = model.update(s, a, r, s_prime, a_prime, done, action_mask)\n",
    "            train_history.append({\n",
    "                'model_name': model_name,\n",
    "                'run': run,\n",
    "                'trial': trial,\n",
    "                'segment': segment,\n",
    "                'episode': episode,\n",
    "                'reward': total_reward,\n",
    "                'alpha': alpha,\n",
    "                'gamma': gamma,\n",
    "                'temp': temp\n",
    "            })\n",
    "\n",
    "        # Testing episodes for this segment\n",
    "        s, info = env.reset()\n",
    "        total_reward = 0\n",
    "        for step in range(200):\n",
    "            a = model.select_action(s, action_mask, greedy=True)\n",
    "            s_prime, r, done, trunc, info = env.step(a)\n",
    "            total_reward += r\n",
    "            if done:\n",
    "                break\n",
    "        test_history.append({\n",
    "            'model_name': model_name,\n",
    "            'run': run,\n",
    "            'trial': trial,\n",
    "            'segment': segment,\n",
    "            'episode': episode,\n",
    "            'reward': total_reward,\n",
    "            'alpha': alpha,\n",
    "            'gamma': gamma,\n",
    "            'temp': temp\n",
    "        })\n",
    "\n",
    "\n",
    "    return train_history, test_history\n",
    "\n",
    "\n",
    "def run_trial_with_index(indexed_param):\n",
    "    index, params = indexed_param\n",
    "    # Call the actual trial function with the provided environment\n",
    "    train_history, test_history = run_trial(params)\n",
    "    return (index, (train_history, test_history))\n",
    "\n",
    "\n",
    "def process_history(history):\n",
    "    dfs = []\n",
    "    for trial, data in enumerate(history):\n",
    "        df = pd.DataFrame(data)\n",
    "        dfs.append(df[['model_name', 'run', 'trial', 'segment', 'episode', 'alpha', 'gamma', 'temp', 'reward']])\n",
    "    \n",
    "    result = pd.concat(dfs).reset_index(drop=True)\n",
    "    result['return'] = (result\n",
    "        .groupby(['run'])['reward']\n",
    "        .apply(np.cumsum, include_groups=True)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    result['step'] = result.groupby(['run', 'trial']).cumcount()\n",
    "    return result\n",
    "\n",
    "\n",
    "def prepare_params(trial_params):\n",
    "    # Create a list of parameters along with original indices\n",
    "    indexed_params_list = list(enumerate(trial_params))\n",
    "    # Shuffle the list to run trials in random order\n",
    "    random.shuffle(indexed_params_list)\n",
    "    return indexed_params_list\n",
    "\n",
    "\n",
    "def sort_and_process_results(results_with_indices):\n",
    "    # Sort the results back into their original order\n",
    "    sorted_results = sorted(results_with_indices, key=lambda x: x[0])\n",
    "    # Extract the results without indices\n",
    "    sorted_results_without_indices = [result for _, result in sorted_results]\n",
    "\n",
    "    train_history, test_history = zip(*sorted_results_without_indices)\n",
    "    train_df = process_history(train_history)\n",
    "    test_df = process_history(test_history)    \n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def run_parallel_trials(trial_params):\n",
    "    indexed_params_list = prepare_params([(param[\"model_name\"], param[\"run\"], param[\"trial\"], param[\"temp\"], \n",
    "                                           param[\"alpha\"], param[\"gamma\"], param[\"n_segments\"], param[\"n_episodes\"]) \n",
    "                                          for param in trial_params])\n",
    "    \n",
    "    pool = Pool(processes=cpu_count())\n",
    "    results_with_indices = list(tqdm(pool.imap(run_trial_with_index, indexed_params_list), total=len(indexed_params_list)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return sort_and_process_results(results_with_indices)\n",
    "\n",
    "\n",
    "def run_sequential_trials(trial_params):\n",
    "    indexed_params_list = prepare_params(trial_params)\n",
    "    \n",
    "    results_with_indices = []\n",
    "    for index, param in tqdm(indexed_params_list, total=len(trial_params), leave=True):\n",
    "        result = run_trial((param[\"model_name\"], param[\"run\"], param[\"trial\"], param[\"temp\"], \n",
    "                            param[\"alpha\"], param[\"gamma\"], param[\"n_segments\"], param[\"n_episodes\"]))\n",
    "        results_with_indices.append((index, result))\n",
    "    \n",
    "    return sort_and_process_results(results_with_indices)\n",
    "\n",
    "\n",
    "# Defining the grids\n",
    "grid_size = 8\n",
    "model_names = ['sarsa', 'expected_sarsa']\n",
    "temps = np.linspace(0.01, 0.8, num=grid_size)\n",
    "alphas = np.logspace(np.log10(0.01), np.log10(1.0), num=grid_size)\n",
    "gammas = [0.95]  # formerly np.linspace(0.85, 1.0, num=100)\n",
    "\n",
    "# Constants\n",
    "n_trials = 10      # How many trials for each combination\n",
    "n_segments = 500   # How many segments in a trial\n",
    "n_episodes = 10    # How many episodes in a segment & how often to run test\n",
    "\n",
    "def sample_params(model_names, temps, alphas, gammas, n_trials):\n",
    "    for run, params in enumerate(product(model_names, temps, alphas, gammas)):\n",
    "        model_name, temp, alpha, gamma = params\n",
    "        for trial in range(n_trials):\n",
    "            yield {\n",
    "                \"model_name\": model_name,\n",
    "                \"run\": run,\n",
    "                \"trial\": trial,\n",
    "                \"temp\": temp,\n",
    "                \"alpha\": alpha,\n",
    "                \"gamma\": gamma,\n",
    "                \"n_segments\": n_segments,\n",
    "                \"n_episodes\": n_episodes\n",
    "            }\n",
    "\n",
    "# Example of sampling a set of parameters\n",
    "trial_params = [params for params in sample_params(model_names, temps, alphas, gammas, n_trials)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb9111bde3840b4812636c28afaef50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists('data/train_df.csv') or not os.path.exists('data/test_df.csv'):\n",
    "    train_df, test_df = run_parallel_trials(trial_params)\n",
    "    train_df.to_csv('data/train_df.csv', index=False)\n",
    "    test_df.to_csv('data/test_df.csv', index=False)\n",
    "else:\n",
    "    train_df = pd.read_csv('data/train_df.csv')\n",
    "    test_df = pd.read_csv('data/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_runs = train_df[train_df.step == train_df.step.max()].sort_values(\"return\", ascending=False).head(10).run.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_df = train_df[train_df.trial < 1]\n",
    "\n",
    "# Assuming `df` is your DataFrame with the data provided\n",
    "# Here we group by 'run' and then plot each group\n",
    "groups = es_df.groupby(['run', 'trial'])\n",
    "\n",
    "plt.figure(figsize=(15, 10))  # Set the figure size as desired\n",
    "\n",
    "for name, group in groups:\n",
    "    model_name, alpha, temp, trial = group.reset_index(drop=True).loc[0,['model_name', 'alpha', 'temp', 'trial']].values\n",
    "    label = f'model={model_name}, alpha={alpha:.2f}, temp={temp:.2f}'\n",
    "    plt.plot(group['step'], group['return'], marker='o', linestyle='-', label=label)\n",
    "\n",
    "plt.xlabel('Step Number')\n",
    "plt.ylabel('Return')\n",
    "plt.title('Return over Steps Grouped by Run Number')\n",
    "plt.legend(title='Run Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_df = test_df[test_df.trial < 1]\n",
    "\n",
    "# Assuming `df` is your DataFrame with the data provided\n",
    "# Here we group by 'run' and then plot each group\n",
    "groups = es_df.groupby(['run', 'trial'])\n",
    "\n",
    "plt.figure(figsize=(15, 10))  # Set the figure size as desired\n",
    "\n",
    "for name, group in groups:\n",
    "    model_name, alpha, temp, trial = group.reset_index(drop=True).loc[0,['model_name', 'alpha', 'temp', 'trial']].values\n",
    "    label = f'model={model_name}, alpha={alpha:.2f}, temp={temp:.2f}'\n",
    "    plt.plot(group['step'], group['return'], marker='o', linestyle='-', label=label)\n",
    "\n",
    "plt.xlabel('Step Number')\n",
    "plt.ylabel('Return')\n",
    "plt.title('Return over Steps Grouped by Run Number')\n",
    "plt.legend(title='Run Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graphs\n",
    "\n",
    "### Graph 1\n",
    "\n",
    "One graph that shows the effect of the parameters on the final **training** performance.\n",
    "\n",
    "The x-axis shows the different parameters (e.g. learning rate), and the y-axis shows the return\n",
    "of the agent (averaged over the last 10 training episodes and the 10 runs); note that this will\n",
    "typically end up as an upside-down U.\n",
    "    \n",
    "**For each algorithm**, the graph should have at least 3 lines (e.g. 3 temperature values) corre-\n",
    "sponding to the choices of the other hyperparameter. There should be at least 6 lines in the\n",
    "graph, and at least 18 different points. Also plot the uncertainty using shading (e.g. using the\n",
    "min and the max return of the 10 runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df` is your DataFrame\n",
    "# Step 1: Filter to only include the last 10 training episodes of each segment\n",
    "df_training_last_10 = train_df[(train_df['episode'] > n_episodes - 11) & (train_df['segment'] == n_segments-1)]\n",
    "\n",
    "# Step 2: Group by model, hyperparameters, and segment, then calculate mean return for the last 10 episodes\n",
    "grouped_means = df_training_last_10.groupby(['model_name', 'alpha', 'temp', 'run', 'trial', 'segment'])['return'].mean().reset_index()\n",
    "\n",
    "# Step 3: Further aggregate across trials to get the mean, min, and max returns for each hyperparameter combination\n",
    "final_stats = grouped_means.groupby(['model_name', 'alpha', 'temp']).agg(\n",
    "    mean_return=('return', 'mean'),\n",
    "    min_return=('return', 'min'),\n",
    "    max_return=('return', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Setup the figure and axes for a 2x2 grid of plots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 14), sharey=True)\n",
    "\n",
    "# Titles for each column (Sarsa vs Expected Sarsa)\n",
    "column_titles = {'sarsa': 'Sarsa', 'expected_sarsa': 'Expected Sarsa'}\n",
    "# Titles for the rows based on the x-axis variable\n",
    "row_titles = ['Alpha', 'Temperature']\n",
    "\n",
    "# Assuming final_stats is correctly structured and contains 'model_name' column with 'Sarsa' and 'Expected Sarsa'\n",
    "for col_index, model in enumerate(column_titles):\n",
    "    model_data = final_stats[final_stats['model_name'] == model]\n",
    "\n",
    "    # First row plots: Alpha values\n",
    "    unique_alphas = model_data['alpha'].unique()\n",
    "    for alpha in unique_alphas:\n",
    "        subset_alpha = model_data[model_data['alpha'] == alpha].sort_values(by='temp')\n",
    "        if not subset_alpha.empty:  # Check if subset is not empty\n",
    "            axes[0, col_index].plot(subset_alpha['temp'], subset_alpha['mean_return'], label=f'Alpha={alpha:.2f}', marker='o')\n",
    "            axes[0, col_index].fill_between(subset_alpha['temp'], subset_alpha['min_return'], subset_alpha['max_return'], alpha=0.2)\n",
    "    axes[0, col_index].set_title(column_titles[model])\n",
    "    axes[0, col_index].set_xlabel('Temperature')\n",
    "    axes[0, col_index].set_ylabel('Average Return')\n",
    "    axes[0, col_index].legend(title='Learning Rate (alpha)')\n",
    "    axes[0, col_index].grid(True)\n",
    "\n",
    "    # Second row plots: Temperature values\n",
    "    unique_temps = model_data['temp'].unique()\n",
    "    for temp in unique_temps:\n",
    "        subset_temp = model_data[model_data['temp'] == temp].sort_values(by='alpha')\n",
    "        if not subset_temp.empty:  # Check if subset is not empty\n",
    "            axes[1, col_index].plot(subset_temp['alpha'], subset_temp['mean_return'], label=f'Temp={temp:.2f}', marker='o')\n",
    "            axes[1, col_index].fill_between(subset_temp['alpha'], subset_temp['min_return'], subset_temp['max_return'], alpha=0.2)\n",
    "    axes[1, col_index].set_xlabel('Learning Rate (alpha)')\n",
    "    axes[1, col_index].set_ylabel('Average Return')\n",
    "    axes[1, col_index].legend(title='Temperature')\n",
    "    axes[1, col_index].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph 2: Test performance\n",
    "\n",
    "The same graph that instead shows the effect of the parameters on the final testing performance. The y-axis should now show the return during the final testing episode, averaged over the 10 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df` is your DataFrame\n",
    "# Step 1: Filter to only include the last 10 training episodes of each segment\n",
    "df_test_last_10 = test_df.loc[test_df.segment == n_segments - 1, :]\n",
    "\n",
    "# Step 2: Group by model, hyperparameters, and segment, then calculate mean return for the last 10 episodes\n",
    "grouped_means = df_test_last_10.groupby(['model_name', 'alpha', 'temp', 'run', 'trial', 'segment'])['return'].mean().reset_index()\n",
    "\n",
    "# Step 3: Further aggregate across trials to get the mean, min, and max returns for each hyperparameter combination\n",
    "final_stats = grouped_means.groupby(['model_name', 'alpha', 'temp']).agg(\n",
    "    mean_return=('return', 'mean'),\n",
    "    min_return=('return', 'min'),\n",
    "    max_return=('return', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Setup the figure and axes for a 2x2 grid of plots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 14), sharey=True)\n",
    "\n",
    "# Titles for each column (Sarsa vs Expected Sarsa)\n",
    "column_titles = {'sarsa': 'Sarsa', 'expected_sarsa': 'Expected Sarsa'}\n",
    "# Titles for the rows based on the x-axis variable\n",
    "row_titles = ['Alpha', 'Temperature']\n",
    "\n",
    "# Assuming final_stats is correctly structured and contains 'model_name' column with 'Sarsa' and 'Expected Sarsa'\n",
    "for col_index, model in enumerate(column_titles):\n",
    "    model_data = final_stats[final_stats['model_name'] == model]\n",
    "\n",
    "    # First row plots: Alpha values\n",
    "    unique_alphas = model_data['alpha'].unique()\n",
    "    for alpha in unique_alphas:\n",
    "        subset_alpha = model_data[model_data['alpha'] == alpha].sort_values(by='temp')\n",
    "        if not subset_alpha.empty:  # Check if subset is not empty\n",
    "            axes[0, col_index].plot(subset_alpha['temp'], subset_alpha['mean_return'], label=f'Alpha={alpha:.2f}', marker='o')\n",
    "            axes[0, col_index].fill_between(subset_alpha['temp'], subset_alpha['min_return'], subset_alpha['max_return'], alpha=0.2)\n",
    "    axes[0, col_index].set_title(column_titles[model])\n",
    "    axes[0, col_index].set_xlabel('Temperature')\n",
    "    axes[0, col_index].set_ylabel('Average Return')\n",
    "    axes[0, col_index].legend(title='Learning Rate (alpha)')\n",
    "    axes[0, col_index].grid(True)\n",
    "\n",
    "    # Second row plots: Temperature values\n",
    "    unique_temps = model_data['temp'].unique()\n",
    "    for temp in unique_temps:\n",
    "        subset_temp = model_data[model_data['temp'] == temp].sort_values(by='alpha')\n",
    "        if not subset_temp.empty:  # Check if subset is not empty\n",
    "            axes[1, col_index].plot(subset_temp['alpha'], subset_temp['mean_return'], label=f'Temp={temp:.2f}', marker='o')\n",
    "            axes[1, col_index].fill_between(subset_temp['alpha'], subset_temp['min_return'], subset_temp['max_return'], alpha=0.2)\n",
    "    axes[1, col_index].set_xlabel('Learning Rate (alpha)')\n",
    "    axes[1, col_index].set_ylabel('Average Return')\n",
    "    axes[1, col_index].legend(title='Temperature')\n",
    "    axes[1, col_index].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph 3: Learning curves\n",
    "\n",
    "Learning curves (mean and standard deviation computed based on the 10 runs) for the best\n",
    "parameter setting for each algorithm. X-axis shows the segment, Y-axis shows return over-\n",
    "time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
